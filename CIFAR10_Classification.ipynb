{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Image Classification - Training your own Convolutional Neural Network #\n",
    "## EECS 16ML, Fall 2021 ##\n",
    "\n",
    "Written by Richard Shuai, Dohyun Cheon, Larry Yan, Tony Shara, Adi Ganapathi. \n",
    "\n",
    "richardshuai@berkeley.edu, dohyuncheon@berkeley.edu, yanlarry@berkeley.edu, anthony.shara@berkeley.edu, avganapathi@berkeley.edu, \n",
    "\n",
    "### Table of contents ###\n",
    "* [Introduction](#introduction)\n",
    "* [Part 1: Introduction to Convolutions](#part1)  \n",
    "◦ Implement the convolution operation  \n",
    "◦ Visualize convolutions with different kernels\n",
    "* [Part 2: Pooling](#part2)  \n",
    "◦ Implement pooling operations  \n",
    "◦ Understand the benefits of pooling  \n",
    "* [Part 3: CIFAR-10 Image Classification](#part3)  \n",
    "◦ Preprocess the dataset  \n",
    "◦ Visualizing filters and activations from a basic CNN  \n",
    "◦ Train your own model and plot learning curves\n",
    "* [Part 4: Tricking a CNN Classifier with Adversarial Examples](#part4)  \n",
    "◦ Generate an adversarial example  \n",
    "◦ Classification of the adversarial example\n",
    "* [Part 5: Implementing a Paper: ResNet](#part5)  \n",
    "◦ Implement a residual block  \n",
    "◦ Train a model using your residual block implementation\n",
    "* [Conclusion and References](#conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from PIL import Image\n",
    "\n",
    "from common_utils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# Introduction\n",
    "\n",
    "In this notebook, you will gain more familiarity with intuition about convolutional neural networks. You are also going to learn to classify images from the CIFAR-10 dataset with a simple convolutional neural network.\n",
    "* First, we will review what a convolution by having you implement a convolution operation. We will then use this implementation to visualize convolutions with different kernels. \n",
    "* Next, you will learn about pooling layers and why they are necessary in convolutional neural networks.\n",
    "* Then, we will visualize the filters and activations in a basic CNN in order to gain an intuition about what a convolutional neural network is actually learning.\n",
    "* You will then implement your own convolutional neural network for image classification.\n",
    "* With this model, you will learn to generate adversarial examples to feed into your newly trained model.\n",
    "* Finally, you will be required to read the ResNet paper and implement a residual block layer.\n",
    "\n",
    "Note that training the models in this notebook may take awhile, so make sure to start early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "# Part 1: Introduction to Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks rely on convolutional layers in order to extract features from images for classification. First, we will introduce the idea of convolutions and how they transform an image. \n",
    "On a 2D input image $I$ and using a kernel $G$ with width $w$ and height $h$, we can perform a discrete 2D-convolution that can be mathematically represented as:\n",
    "\n",
    "$$\n",
    "(I ∗ G)[x, y] = \\sum_{k=1}^{w} \\sum_{l=1}^{h} I[x + k, y + l]G[k, l]\n",
    "$$\n",
    "  \n",
    "Visually, the convolution operation looks like this: ([source](https://mlnotebook.github.io/post/CNN1/))\n",
    "<img src=\"./visuals/convSobel.gif\" width=500 align=\"center\"/>\n",
    "\n",
    "Note that the convolution output dimensions depends on the dimensions of both the image and the kernel.    \n",
    "\n",
    "In convolutional neural networks, the input image often has multiple channels, so our kernel will have multiple channels to match the input. We sum across the channels so that the convolution operation becomes:\n",
    "$$\n",
    "(I ∗ G)[x, y] = \\sum_c \\sum_{k=1}^{w} \\sum_{l=1}^{h} I[x + k, y + l, c]G[k, l, c]\n",
    "$$\n",
    "\n",
    "Visually, the convolution will look like this: ([source](https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37))\n",
    "<img src=\"./visuals/conv_multiple_channels.png\" width=300 height=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate what different convolutional filters look like. In order to do so, you will need fill in the code below to implement a naive version of the convolution operation with stride and padding. Do not worry about efficiency for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(X, kernel, stride=1, padding=(0, 0)):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "     - X: Input image. Numpy array of shape (H, W, C).\n",
    "     - kernel: Kernel. Numpy array of shape (H_k, W_k, C).\n",
    "     - stride: Stride in both the y- and x-directions. Positive integer.\n",
    "     - padding: Symmetric zero-padding specification for both the x- and y-directions. Tuple where padding[0]\n",
    "                 and padding[1] specify padding in the y-direction and x-direction respectively.\n",
    "    \"\"\"\n",
    "    # Expand for single channel dimension\n",
    "    if X.ndim == 2:\n",
    "        X = X[..., None]\n",
    "    if kernel.ndim == 2:\n",
    "        kernel = kernel[..., None]\n",
    "    \n",
    "    assert len(X.shape) == 3, 'Expected X to have 3 dimensions, but got shape {}'.format(X.shape)\n",
    "    assert len(kernel.shape) == 3, 'Expected kernel to have 3 dimensions, but got shape {}'.format(kernel.shape)\n",
    "    assert len(padding) == 2, 'Expected padding to be length 2, but got length {}'.format(len(padding))\n",
    "    \n",
    "    \n",
    "    H, W, C = X.shape\n",
    "    H_k, W_k, _ = kernel.shape\n",
    "    pad_y, pad_x = padding\n",
    "    \n",
    "    out = None\n",
    "    ### BEGIN CODE ###\n",
    "\n",
    "    ### END CODE ###\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for conv2d implementation\n",
    "X = np.random.randint(0, 50, size=(7, 7, 1)).astype(np.float32)\n",
    "K = np.random.randint(0, 5, size=(3, 3, 1)).astype(np.float32)\n",
    "\n",
    "# With stride=1, padding=(0, 0)\n",
    "conv_naive = conv2d(X, K)\n",
    "conv_tf = tf.nn.conv2d(X[None], K[..., None], strides=1, padding='VALID').numpy().squeeze()\n",
    "assert np.all(conv_naive == conv_tf)\n",
    "\n",
    "# With stride=2, padding=(0, 0)\n",
    "conv_naive = conv2d(X, K, stride=2)\n",
    "conv_tf = tf.nn.conv2d(X[None], K[..., None], strides=2, padding='VALID').numpy().squeeze()\n",
    "assert np.all(conv_naive == conv_tf)\n",
    "\n",
    "# With stride=2, padding=(2, 3)\n",
    "conv_naive = conv2d(X, K, stride=2, padding=(2, 3))\n",
    "conv_tf = tf.nn.conv2d(X[None], K[..., None], strides=2, padding=((0, 0), (2, 2), (3, 3), (0, 0))).numpy().squeeze()\n",
    "assert np.all(conv_naive == conv_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize convolutions with different kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels for Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the convolution operation, we will use this to visualize the effects of convolution with certain kernels. Here, we visualize the effects of convolving 4 different filters on the cameraman image:  \n",
    "\n",
    "<img src=\"./visuals/camera_man.png\" width=200 align=\"center\"/>\n",
    "\n",
    "The kernels that we will be visualizing are listed below.  \n",
    "Identity Kernel:\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "$  \n",
    "Edge detection kernel 1:\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "-1 & -1 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 1 & 1 \n",
    "\\end{bmatrix}\n",
    "$  \n",
    "Edge detection kernel 2:\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \n",
    "\\end{bmatrix}\n",
    "$  \n",
    "Edge detection kernel 3:\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "-1 & -1 & -1 \\\\\n",
    "-1 & 8 & -1 \\\\\n",
    "-1 & -1 & -1 \n",
    "\\end{bmatrix}\n",
    "$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.asarray(Image.open('./visuals/camera_man.png'))\n",
    "\n",
    "kernels = np.array([[[0, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 0]],\n",
    "                    \n",
    "                [[-1, -1, -1],\n",
    "                [0, 0, 0],\n",
    "                [1, 1, 1]],\n",
    "                \n",
    "                [[-1, 0, 1],\n",
    "                [-1, 0, 1],\n",
    "                [-1, 0, 1]],\n",
    "                \n",
    "                [[-1, -1, -1],\n",
    "                [-1, 8, -1],\n",
    "                [-1, -1, -1]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_imgs = []\n",
    "labels = ['Identity kernel', 'Edge detection 1', 'Edge detection 2', 'Edge detection 3']\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    convolved_img = conv2d(img, kernel)\n",
    "    convolved_imgs.append(convolved_img)\n",
    "    \n",
    "plot_labeled_image_grid(np.array(convolved_imgs), labels, cmap='gray', sizes=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe what you notice about the different convolution outputs. How do the edge detection kernels differ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian blurring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick demonstration, beyond just edge detection, convolving images with a kernel can also achieve a blurring effect. Here, we apply a Gaussian blur by convolving a certain matrix over the image. This matrix is based on the probability density function for a 2D Gaussian distribution with mean $0$ and standard deviation $\\sigma$:  \n",
    "\n",
    "$$ f(x, y) = \\frac{1}{2\\pi \\sigma^2}e^{-\\frac{x^2 + y^2}{2\\sigma^2}}$$.  \n",
    "A kernel is obtained from this density function by integrating over each pixel. Doing this with $\\sigma=1$ for a  5x5 yields this discrete approximation:  \n",
    "\n",
    "$$ \\frac{1}{256} \n",
    "\\begin{bmatrix} \n",
    "1 & 4 & 6 & 4 & 1 \\\\\n",
    "4 & 16 & 24 & 16 & 4 \\\\ \n",
    "6 & 24 & 36 & 24 & 6 \\\\\n",
    "4 & 16 & 24 & 16 & 4 \\\\\n",
    "1 & 4 & 6 & 4 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_kernel = np.array([[1, 4, 6, 4, 1],\n",
    "                            [4, 16, 24, 16, 4], \n",
    "                            [6, 24, 36, 24, 6],\n",
    "                            [4, 16, 24, 16, 4],\n",
    "                            [1, 4, 6, 4, 1]]) / 256\n",
    "\n",
    "# Blurred image\n",
    "blurred_img = conv2d(img, gaussian_kernel)\n",
    "\n",
    "# Plot images side by side\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "s1 = fig.add_subplot(1, 2, 1)\n",
    "s1.set_title(r'Original Image')\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "s2 = fig.add_subplot(1, 2, 2)\n",
    "s2.set_title(r'Gaussian Blur with $\\sigma=1$')\n",
    "plt.imshow(blurred_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "# Part 2: Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers are also an essential component of convolutional neural networks, both for parameter efficiency and for extracting information from larger contexts. Pooling reduces the size of an image or feature map by performing a function over slices of an image. The most commonly used type of pooling is **max pooling**, in which the maximum element is taken over the size of the filter. For example, it is common to use a max pooling with a filter of size 2x2 and stride of 2. This operation is depicted below ([source](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling)):\n",
    "\n",
    "<img src=\"./visuals/MaxpoolSample2.png\" width=400 align=\"center\"/>\n",
    "\n",
    "In the following section, you will be required to implement some of the types of pooling operation before using your function to visualize how pooling transforms an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Pooling Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code below to implement a pooling operation on an image. You will be required to implement both max pooling and average pooling, in average pooling, you simply take the average over the elements in the filter instead of the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, stride, pool_type='max'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "     - X: Input image. Numpy array of shape (H, W, C).\n",
    "     - pool_size: Size of the pooling filter in both the y- and x- directions. Positive integer.\n",
    "     - stride: Stride in both the y- and x-directions. Positive integer.\n",
    "     - pool_type: either 'max' or 'average' to denote pool type. String.\n",
    "    \"\"\"\n",
    "    pool_types = {'max', 'average'}\n",
    "    # Expand for single channel dimension\n",
    "    if X.ndim == 2:\n",
    "        X = X[..., None]\n",
    "    \n",
    "    assert len(X.shape) == 3, 'Expected X to have 3 dimensions, but got shape {}'.format(X.shape)\n",
    "    assert pool_type in pool_types, 'Pool type not implemented: {}'.format(pool_type)\n",
    "    \n",
    "    H, W, C = X.shape\n",
    "    \n",
    "    out = None\n",
    "    ### BEGIN CODE ###\n",
    "\n",
    "    ### END CODE ###\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for pool2d implementation\n",
    "X = np.random.randint(0, 20, size=(7, 7, 3)).astype(np.float32)\n",
    "\n",
    "# With pool_size=2, stride=2, pool_type='max'\n",
    "pool_naive = pool2d(X, pool_size=2, stride=2, pool_type='max')\n",
    "pool_tf = tf.nn.max_pool2d(X[None], ksize=2, strides=2, padding='VALID').numpy()\n",
    "assert np.all(pool_naive == pool_tf)\n",
    "\n",
    "# With pool_size=2, stride=2, pool_type='average'\n",
    "pool_naive = pool2d(X, pool_size=2, stride=2, pool_type='average')\n",
    "pool_tf = tf.nn.avg_pool2d(X[None], ksize=2, strides=2, padding='VALID').numpy()\n",
    "assert np.all(pool_naive == pool_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the Benefits of Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling with filter size 2x2 and a stride of 2 reduces the number of pixels in an image by a factor of 4 while keeping the main features of the image. This provides several benefits:\n",
    "* By reducing the size of the image, successive convolution operations are not as expensive. This is especially important in deeper networks, where the number of channels of each feature map is increased in order to extract more features from an image. \n",
    "* By downsampling the image, convolutions applied over the downsampled image can have a larger receptive field. In other words, they can obtain context from a larger area.\n",
    "* Because max pooling extracts the dominant value in each region, it can extract sharp features such as edges and other important features. Max pooling has empirically been shown to improve results in many CNN architectures.\n",
    "\n",
    "As you can see below, max pooling can be necessary to keep feature maps to a reasonable size, especially with a large number of channels ([source](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling)).\n",
    "<img src=\"./visuals/MaxpoolSample.png\" width=400 align=\"center\"/>\n",
    "\n",
    "In this section, we will use your pooling function to visualize the outcomes of repeatedly applying pooling on both a greyscale of a boat and color image of fruits.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./visuals/boat.png\" width=200/> </td>\n",
    "<td> <img src=\"./visuals/fruits.png\" width=200/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in boat and fruits images.\n",
    "boat_img = np.asarray(Image.open('./visuals/boat.png'))\n",
    "fruits_img = np.asarray(Image.open('./visuals/fruits.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greyscale image: boats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we repeatedly apply pooling to the greyscale image with a 2x2 filter size and a stride of 2. We plot the outputs from average and max pooling at each step. We do the same with the fruits image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pools = 5\n",
    "\n",
    "fig=plt.figure(figsize=(5 * num_pools, 10))\n",
    "rows, columns = 2, num_pools\n",
    "\n",
    "avg_pooled_img = boat_img\n",
    "max_pooled_img = boat_img\n",
    "\n",
    "for i in range(num_pools):\n",
    "    # Plot max pool\n",
    "    s = fig.add_subplot(rows, columns, i+1)\n",
    "    s.set_title(\"Max pool #{}\".format(i+1))\n",
    "    max_pooled_img = pool2d(max_pooled_img, pool_size=2, stride=2, pool_type='max')\n",
    "    plt.imshow(max_pooled_img, cmap='gray')\n",
    "    \n",
    "    # Plot ground truth\n",
    "    s = fig.add_subplot(rows, columns, i+columns+1)\n",
    "    s.set_title(\"Average pool #{}\".format(i+1))\n",
    "    avg_pooled_img = pool2d(avg_pooled_img, pool_size=2, stride=2, pool_type='average')\n",
    "\n",
    "    plt.imshow(avg_pooled_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color image: Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(5 * num_pools, 10))\n",
    "rows, columns = 2, num_pools\n",
    "\n",
    "avg_pooled_img = fruits_img\n",
    "max_pooled_img = fruits_img\n",
    "\n",
    "for i in range(num_pools):\n",
    "    # Plot max pool\n",
    "    s = fig.add_subplot(rows, columns, i+1)\n",
    "    s.set_title(\"Max pool #{}\".format(i+1))\n",
    "    max_pooled_img = pool2d(max_pooled_img, pool_size=2, stride=2, pool_type='max')\n",
    "    plt.imshow(normalize(max_pooled_img))\n",
    "    \n",
    "    # Plot ground truth\n",
    "    s = fig.add_subplot(rows, columns, i+columns+1)\n",
    "    s.set_title(\"Average pool #{}\".format(i+1))\n",
    "    avg_pooled_img = pool2d(avg_pooled_img, pool_size=2, stride=2, pool_type='average')\n",
    "\n",
    "    plt.imshow(normalize(avg_pooled_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe the differences you notice between max pooling and average pooling. Looking at the scales of the images, can you see why pooling is helpful for increasing the receptive field of convolutions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "\n",
    "# Part 3: CIFAR-10 Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a better understanding of how convolutions and pooling work and how they transform images, we will learn how to use a convolutional neural network to extract features from an image and perform image classification on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the CIFAR-10 Dataset\n",
    "(X, Y), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a few images from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll plot some of the images to understand what we're actually classifying. The possible classes are given in the label_names array. Feel free to change num_visualize to see more images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "# Input the number of images to visualize\n",
    "num_visualize = 20\n",
    "\n",
    "# Construct a grid to plot images\n",
    "labels = label_names[Y[:num_visualize].squeeze()] # Get label names in English from their 0-9 encodings\n",
    "plot_labeled_image_grid(X[:num_visualize], labels, title='CIFAR 10 Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the images from the CIFAR-10 dataset are unfortunately low resolution because each image is only 32x32 pixels. These images are small to help us focus on the classification task without worrying too much about computing power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train, Validation, and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code to split the dataset into train and validation sets. Here, we will use a 90:10 split to create our train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.9\n",
    "NUM_TRAIN_SAMPLES = int(TRAIN_SPLIT * X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X, Y, num_train_samples):\n",
    "    N = X.shape[0]\n",
    "\n",
    "    ### BEGIN CODE ###\n",
    "\n",
    "    ### END CODE ###\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset with our function\n",
    "X_train, Y_train, X_val, Y_val = train_val_split(X, Y, NUM_TRAIN_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for shapes\n",
    "assert(X_train.shape == (45000, 32, 32, 3))\n",
    "assert(X_val.shape == (5000, 32, 32, 3))\n",
    "assert(Y_train.shape == (45000, 1))\n",
    "assert(Y_val.shape == (5000, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the images in our dataset. There are several ways you can do this. We have provided some examples below.\n",
    "* Normalize the data from 0 to 1. \n",
    "* Subtract the mean of the training data\n",
    "* Z-score standardization\n",
    "* Dividing by 255 (for images ranging from 0 to 255)\n",
    "\n",
    "Remember to apply the same preprocessing to the test and validation datasets as you do the train datasets. (For example, if the mean is calculated across the training data, subtract that same value from the validation and test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing filters and activations from a basic CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we define and train a basic CNN and visualize the learned filters from the first layer as well as layer activations when classifying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=9, padding='same', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this classification task, we use categorical cross entropy loss. In Keras, since our data is not one-hot encoded, we use sparse categorical cross entropy loss, but this is equivalent to categorical cross entropy loss for one-hot encoded labels. This loss for each sample is defined as: \n",
    "$$L(x) = \\displaystyle\\sum_{k=1}^C -y_{k} \\cdot log(\\sigma(s)_k)$$\n",
    "\n",
    "* $y_k = 1$ if the sample belongs to class $k$ and $y_k = 0$ is it does not belong to class $k$.\n",
    "* $s$ is the score obtained from the last layer of the network before the activation function.\n",
    "* $\\sigma$ is the softmax activation function:\n",
    "$$\\sigma(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^{C} e^{z_j}}$$  \n",
    "\n",
    "The loss is therefore\n",
    "$$Loss = \\frac{1}{N} \\displaystyle\\sum_{i=1}^{N} \\displaystyle\\sum_{k=1}^C -y_{i, k} \\cdot log(\\sigma(s_i)_k) $$  \n",
    "\n",
    "where we average over the batch with batch size $N$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compile and view a summary of our model. We can observe that most of the weights in our model come from the fully connected layers at the end, and that the pooling layers do not require any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize learned filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, we can visualize the filters from the first convolutional layer to see what features the filters are detecting. As you should see, some of these filters look like edge-detecting filters.   \n",
    "Note that here, we only extract the filters from the first convolutional layer because it is generally more difficult to interpret the convolutional filters from deeper layers. This is because these deeper layers act on encoded representations of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the layers of the model in a list\n",
    "layers = model.layers\n",
    "\n",
    "# Extract the filters from the first convolutional layer\n",
    "layer_0 = layers[0]\n",
    "filters, biases = layer_0.get_weights()\n",
    "filters = normalize(filters)\n",
    "\n",
    "plot_image_grid(filters.transpose(3, 0, 1, 2), title='Learned Convolutional Filters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do any of these filters appear meaningful or recognizable?** *(Hint: you may recognize some filters from earlier in this notebook)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize activations for certain images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the activations from the network when it makes predictions on certain images. This helps us understand how each filter encodes the image and transforms it through successive convolutional layers. Here, we demonstrate a visualization for the first layer activations. Fill in the code to visualize activations from the second and third convolutional layers. \n",
    "\n",
    "*(Hint: Inspect model.layers in order to understand the outputs of the activation_model.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2 # Change k to visualize different images in the training dataset.\n",
    "\n",
    "img = X_train[k]\n",
    "label = label_names[Y_train[k].squeeze()]\n",
    "\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "plt.imshow(img)\n",
    "plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(model.input, layer_outputs)\n",
    "activations = activation_model(img[np.newaxis, ...])\n",
    "\n",
    "first_conv_activations = activations[0]\n",
    "\n",
    "plot_image_grid(tf.squeeze(tf.transpose(first_conv_activations, [3, 0, 1, 2])), title='First Convolutional Layer Activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv_activations = None\n",
    "third_conv_activations = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###\n",
    "\n",
    "plot_image_grid(tf.squeeze(tf.transpose(second_conv_activations, [3, 0, 1, 2])), title='Second Convolutional Layer Activations')\n",
    "plot_image_grid(tf.squeeze(tf.transpose(third_conv_activations, [3, 0, 1, 2])), title='Third Convolutional Layer Activations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe what you notice about the activations. How is the image transformed over successive layers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Own Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've developed a better intuitive understanding of how convolutional neural networks learn, it's time for you to implement a better architecture that can achieve a higher validation loss. Train a model to achieve a validation loss of at least 70% on CIFAR-10. \n",
    "\n",
    "Some tricks that might be helpful:\n",
    "* Common kernel sizes are 3x3 or 5x5. Remember that larger kernel sizes can require a much larger number of parameters.\n",
    "* Batch normalization can vastly speed up training by rescaling the inputs to each layer. It can also be act to regularize your network. You can read more about it here: https://arxiv.org/abs/1502.03167\n",
    "* Architectures commonly make use of a CONV -> BATCH_NORM -> ACTIVATION -> CONV -> BATCH_NORM -> ACTIVATION -> POOLING pattern.\n",
    "* Other ideas to test out include changing hyperparameters such as stride, adding dropout layers, and different optimization algorithms (https://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "You are given a lot of freedom to write the code for your architecture. Feel free to change any starter code if you feel like it's necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "## END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the train loss and validation loss as a function of the number of epochs trained.\n",
    "\n",
    "*(Hint: the history returned from model.fit may be helpful here.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the final loss on our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the model on the test set to get the final test accuracy for the model. Note that we only evaluate on the test set once, at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print('Final test set accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "\n",
    "# Part 4: Tricking a CNN Classifier with Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, CNNs are capable of classifying images with high accuracy. However, it turns out that it is very easy to \"trick\" a trained CNN classifier with *adversarial examples*. By taking a correctly classified image and adding a small but directed perturbation to the image, we can force the CNN to misclassify the image with high confidence. In this section, we will briefly demonstrate how we can cause your newly trained model to misclassify images with methods based on this paper: https://arxiv.org/abs/1412.6572. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change that we will be making is as follows:\n",
    "$$\\vec{x}_{adversarial} = \\vec{x} + \\epsilon \\cdot sign(\\nabla_x J(\\theta, \\vec{x}, y))$$  \n",
    "where $J(\\theta, \\vec{x}, y))$ is the loss resulting from evaluating the model on an input image $\\vec{x}$ with model parameters $\\theta$, and $\\epsilon$ is a small constant. Notice that we are taking the gradient with respect to the input image $x$ this time. Intuitively, this means that we are changing the image by making a small step in the direction that maximizes the loss the most under a max-norm constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code below to implement this update. You will need to compute the gradient of the loss with respect to the model input and set it to grad_x.  \n",
    "*(Hint: [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape) may be helpful here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an adversarial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_example(model, x, y_true, eps=0.01):\n",
    "    if len(x.shape) == 3:\n",
    "        x = x[np.newaxis, ...]\n",
    "\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    \n",
    "    grad_x = None\n",
    "    adversarial_example = None\n",
    "\n",
    "    ### BEGIN CODE ###\n",
    "\n",
    "    ### END CODE ### \n",
    "    \n",
    "    return (grad_x, adversarial_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of our example, we want to make sure we perturb an image that the model would ordinarily be able to correctly classify. Obtain the index of such a correctly classified image in X_train and set it to variable idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original image and your model's prediction\n",
    "img = X_train[idx]\n",
    "y_true = Y_train[idx].squeeze()\n",
    "\n",
    "plt.title(label_names[y_true])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this correctly classified image, fill in the code below to generate an adversarial example that your model is likely to misclassify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.01\n",
    "grad_x = None\n",
    "adversarial_img = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the adversarial example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the forward pass of the model to find the prediction it makes on the adversarial input and set the label to label_new. You may need to slightly tune $\\epsilon$ from above or change the original image in order to misclassify the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_new = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "ax1.set_title('Original prediction: {}'.format(label_names[y_true]), fontsize=16)\n",
    "ax1.imshow(img)\n",
    "\n",
    "ax2.set_title(r'$\\vec{\\epsilon} \\cdot sign(\\nabla_x J(\\theta, \\vec{x}, y))$', fontsize=16)\n",
    "ax2.imshow(normalize(tf.squeeze(grad_x)))\n",
    "\n",
    "ax3.set_title('Adversarial prediction: {}'.format(label_new), fontsize=16)\n",
    "ax3.imshow(normalize(tf.squeeze(adversarial_img)))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the two images look identical to the human eye. This demonstrates that although CNNs appear to classify images in a very intuitive way by extracting recognizable features, the model can still fail to classify images that are perturbed in a certain way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part5\"></a>\n",
    "# Part 5: Implementing a Paper: ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a residual block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing machine learning models, it is valuable practice to be able to read a paper, understand it conceptually, and then to actually implement its architecture. Here, we will implement a key component of the architecture from the ResNet paper: residual blocks. \n",
    "\n",
    "Read through the ResNet paper, and then fill in the code to implement a residual block with two layers.  \n",
    "The ResNet paper can be found here: https://arxiv.org/abs/1512.03385\n",
    "\n",
    "Below, we've included a diagram from the picture for what specifically you should implement: use convolutional layers for the weight layers, and if the dimensions of $\\vec{x}$ and $\\mathcal{F}$ do not match, use a 1x1 convolution as the linear projection to match the dimensions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"visuals/resnet_building_block.png\" alt=\"Drawing\" style=\"width: 50em;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the intuition behind using residual blocks in a neural network? Why are they useful?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        ### BEGIN CODE ###\n",
    "\n",
    "        ### END CODE ###\n",
    "        \n",
    "    def call(self, x):\n",
    "        ### BEGIN CODE ###\n",
    "\n",
    "        ### END CODE ###\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sanity checking for the shapes of your outputs ###\n",
    "x = tf.random.uniform([1, 5, 5, 3])\n",
    "\n",
    "residual_block_1 = ResidualBlock(in_channels=3, out_channels=3, kernel_size=3)\n",
    "residual_block_2 = ResidualBlock(in_channels=3, out_channels=7, kernel_size=3)\n",
    "\n",
    "assert tuple(tf.shape(residual_block_1(x))) == (1, 5, 5, 3)\n",
    "assert tuple(tf.shape(residual_block_2(x))) == (1, 5, 5, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model using your residual block implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can test your residual block implementation by directly using it to train a model. As you implement your model, you should keep in mind that residual blocks tend to shine the most in deeper networks.    \n",
    "\n",
    "Again, we have provided starter code for the loss function and fitting, but feel free to change the code if you need to. Achieve a validation accuracy of over 70%, but do not worry about outperforming the model you trained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "## END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report your final loss on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print('Final test set accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# Conclusion\n",
    "Great job! You've implemented a convolutional layers and successfully built your own CNN to classify images, learning how you can visualize the filters and activations of a trained model along the way. You also saw a bit about how small but selective perturbations can cause your CNN to misclassify relatively easily. Finally, you were able to practice reading and implementing a machine learning research paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Sergey Ioffe, Christian Szegedy)  \n",
    "https://arxiv.org/abs/1502.03167\n",
    "\n",
    "An overview of gradient descent optimization algorithms (Sebastian Ruder)  \n",
    "https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "\n",
    "Explaining and Harnessing Adversarial Examples (Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy)  \n",
    "https://arxiv.org/abs/1412.6572\n",
    "\n",
    "Deep Residual Learning for Image Recognition (Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun)  \n",
    "https://arxiv.org/abs/1512.03385"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

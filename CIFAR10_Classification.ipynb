{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Image Classification - Training your own Convolutional Neural Network #\n",
    "## EECS 16ML, Fall 2021 ##\n",
    "\n",
    "Written by Richard Shuai, Dohyun Cheon, Larry Yan, Tony Shara, Adi Ganapathi, \n",
    "\n",
    "richardshuai@berkeley.edu, dohyuncheon@berkeley.edu, yanlarry@berkeley.edu, anthony.shara@berkeley.edu, avganapathi@berkeley.edu, \n",
    "\n",
    "### Table of contents ###\n",
    "* [Introduction](#introduction)  \n",
    "* [Part 1: CIFAR-10 Image Classification](#part1)  \n",
    "◦ Dataset Preprocessing  \n",
    "◦ Visualizing a basic CNN  \n",
    "◦ Train your own model  \n",
    "* [Part 2: Tricking a CNN Classifier with Adversarial Examples](#part2)  \n",
    "◦ Dataset Preprocessing  \n",
    "* [Part 3: Implementing a Paper: ResNet](#part3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# Introduction\n",
    "\n",
    "In this notebook, you will gain more familiarity with intuition about convolutional neural network. You are also going to learn to classify images from the CIFAR-10 dataset with a simple convolutional neural network.\n",
    "* First, we will visualize the filters and activations in the network in order to gain an intuition about what the convolutional neural network is actually learning.\n",
    "* You will then implement your own convolutional neural network for image classification.\n",
    "* With this model, you will learn to generate adversarial examples to feed into your newly trained model.\n",
    "* Finally, you will be required to read the ResNet paper and implement a residual block layer.\n",
    "\n",
    "Note that training the models in this notebook may take awhile, so make sure to start early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "# Part 1: CIFAR-10 Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from common_utils import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, Y), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a few images from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll plot some of the images to understand what we're actually classifying. The possible classes are given in the label_names array. Feel free to change num_visualize to see more images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "\n",
    "# Input the number of images to visualize\n",
    "num_visualize = 20\n",
    "\n",
    "# Construct a grid to plot images\n",
    "labels = label_names[Y[:num_visualize].squeeze()] # Get label names in English from their 0-9 encodings\n",
    "plot_labeled_image_grid(X[:num_visualize], labels, title='CIFAR 10 Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the images from the CIFAR-10 dataset are unfortunately low resolution because each image is only 32x32 pixels. These images are small to help us focus on the classification task without worrying too much about computing power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Validation, and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code to split the dataset into train and validation sets. Here, we will use a 90:10 split to create our train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.9\n",
    "NUM_TRAIN_SAMPLES = int(TRAIN_SPLIT * X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X, Y, num_train_samples):\n",
    "    N = X.shape[0]\n",
    "\n",
    "    ### BEGIN CODE ###\n",
    "\n",
    "    ### END CODE ###\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset with our function\n",
    "X_train, Y_train, X_val, Y_val = train_val_split(X, Y, NUM_TRAIN_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for shapes\n",
    "assert(X_train.shape == (45000, 32, 32, 3))\n",
    "assert(X_val.shape == (5000, 32, 32, 3))\n",
    "assert(Y_train.shape == (45000, 1))\n",
    "assert(Y_val.shape == (5000, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the images in our dataset. There are several ways you can do this:\n",
    "* Normalize the data from 0 to 1. \n",
    "* Subtract the mean of the training data\n",
    "* Z-score standardization\n",
    "* Dividing by 255 (for images ranging from 0 to 255)\n",
    "\n",
    "Remember to apply the same preprocessing to the test and validation datasets as you do the train datasets. (For example, if the mean is calculated across the training data, subtract that same value from the validation and test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we define and train a basic CNN and visualize the learned filters from the first layer as well as layer activations when classifying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=9, padding='same', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this classification task, we use categorical cross entropy loss. In Keras, since our data is not one-hot encoded, we use sparse categorical cross entropy loss, but this is equivalent to categorical cross entropy loss for one-hot encoded labels. This loss for each sample is defined as: \n",
    "$$L(x) = \\displaystyle\\sum_{k=1}^C -y_{k} \\cdot log(\\sigma(s)_k)$$\n",
    "\n",
    "* $y_k = 1$ if the sample belongs to class $k$ and $y_k = 0$ is it does not belong to class $k$.\n",
    "* $s$ is the score obtained from the last layer of the network before the activation function.\n",
    "* $\\sigma$ is the softmax activation function:\n",
    "$$\\sigma(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^{C} e^{z_j}}$$  \n",
    "\n",
    "The loss is therefore\n",
    "$$Loss = \\frac{1}{N} \\displaystyle\\sum_{i=1}^{N} \\displaystyle\\sum_{k=1}^C -y_{i, k} \\cdot log(\\sigma(s_i)_k) $$  \n",
    "\n",
    "where we average over the batch with batch size $N$.\n",
    "We recommend you take some time to understand this loss intuitively by thinking about the 2-class case, with $C=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize learned filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, we can visualize the filters from the first convolutional layer to see what features the filters are detecting. As you should see, some of these filters look like edge-detecting filters.   \n",
    "Note that here, we only extract the filters from the first convolutional layer because it is generally more difficult to interpret the convolutional filters from deeper layers. This is because these deeper layers act on encoded representations of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    a = np.min(x)\n",
    "    b = np.max(x)\n",
    "    return (x - a) / (b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the layers of the model in a list\n",
    "layers = model.layers\n",
    "\n",
    "# Extract the filters from the first convolutional layer\n",
    "layer_0 = layers[0]\n",
    "filters, biases = layer_0.get_weights()\n",
    "filters = normalize(filters)\n",
    "\n",
    "plot_image_grid(filters.transpose(3, 0, 1, 2), title='Learned Convolutional Filters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do any of these filters appear meaningful or recognizable? You may have seen similar filters from lecture or the notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Answer: </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize activations for certain images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the activations from the network when it makes predictions on certain images. This helps us understand how each filter encodes the image and transforms it through successive convolutional layers. Here, we demonstrate a visualization for the first layer activations. Fill in the code to visualize activations from the second and third convolutional layers. \n",
    "\n",
    "Hint: Insepct model.layers in order to understand the outputs of the activation_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2 # Change k to visualize different images in the training dataset.\n",
    "\n",
    "img = X_train[k]\n",
    "label = label_names[Y_train[k].squeeze()]\n",
    "\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "plt.imshow(img)\n",
    "plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(model.input, layer_outputs)\n",
    "activations = activation_model(img[np.newaxis, ...])\n",
    "\n",
    "first_conv_activations = activations[0]\n",
    "\n",
    "plot_image_grid(tf.squeeze(tf.transpose(first_conv_activations, [3, 0, 1, 2])), title='First Convolutional Layer Activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv_activations = None\n",
    "third_conv_activations = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###\n",
    "\n",
    "plot_image_grid(tf.squeeze(tf.transpose(second_conv_activations, [3, 0, 1, 2])), title='Second Convolutional Layer Activations')\n",
    "plot_image_grid(tf.squeeze(tf.transpose(third_conv_activations, [3, 0, 1, 2])), title='Third Convolutional Layer Activations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what you notice about the activations. How is the image transformed over successive layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Answer: </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Own Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've developed a better intuitive understanding of how convolutional neural networks learn, it's time for you to implement a better architecture that can achieve a higher validation loss. Train a model to achieve a validation loss of at least 70% on CIFAR-10. \n",
    "\n",
    "Some tricks that might be helpful:\n",
    "* Common kernel sizes are 3x3 or 5x5. Remember that larger kernel sizes can require a much larger number of parameters.\n",
    "* Batch normalization can vastly speed up training by rescaling the inputs to each layer. It can also be act to regularize your network. You can read more about it here: https://arxiv.org/abs/1502.03167\n",
    "* Architectures commonly make use of a CONV -> BATCH_NORM -> ACTIVATION -> CONV -> BATCH_NORM -> ACTIVATION -> POOLING pattern.\n",
    "* Other ideas to test out include changing hyperparameters such as stride, adding dropout layers, and different optimization algorithms (https://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "You are given a lot of freedom to write the code for your architecture. Feel free to change any starter code if you feel like it's necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "## END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the train loss and validation loss as a function of the number of epochs trained.\n",
    "\n",
    "Hint: the history returned from model.fit may be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the final loss on our test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model once on the test set to achieve the final test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "\n",
    "# Part 2: Tricking a CNN Classifier with Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, CNNs are capable of classifying images with high accuracy. However, it turns out that it is very easy to \"trick\" a trained CNN classifier with *adversarial examples*. By taking a correctly classified image and adding a small but directed perturbation to the image, we can force the CNN to misclassify the image with high confidence. In this section, we will briefly demonstrate how we can cause your newly trained model to misclassify images with methods based on this paper: https://arxiv.org/abs/1412.6572. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change that we will be making is as follows:\n",
    "$$\\vec{x}_{adversarial} = \\vec{x} + \\epsilon \\cdot sign(\\nabla_x J(\\theta, \\vec{x}, y))$$  \n",
    "where $J(\\theta, \\vec{x}, y))$ is the loss resulting from evaluating the model on an input image $\\vec{x}$ with model parameters $\\theta$, and $\\epsilon$ is a small constant. Notice that we are taking the gradient with respect to the input image $x$ this time. Intuitively, this means that we are changing the image by making a small step in the direction that maximizes the loss the most under a max-norm constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the code below to implement this update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an adversarial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_example(model, x, y_true, eps=0.01):\n",
    "    if len(x.shape) == 3:\n",
    "        x = x[np.newaxis, ...]\n",
    "\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y_pred = model(x)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n",
    "    grad_x = tape.gradient(loss, x)\n",
    "    \n",
    "    adversarial_example = None\n",
    "\n",
    "    ### BEGIN CODE ###\n",
    "\n",
    "    ### END CODE ### \n",
    "    \n",
    "    return (grad_x, adversarial_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of our example, we want to make sure we perturb an image that the model would ordinarily be able to correctly classify. Obtain the index of such a correctly classified image in X_train and set it to variable idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original image and your model's prediction\n",
    "img = X_train[idx]\n",
    "y_true = Y_train[idx].squeeze()\n",
    "\n",
    "plt.title(label_names[y_true])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this correctly classified image, fill in the code below to generate an adversarial example that your model is likely to misclassify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.01\n",
    "grad_x = None\n",
    "adversarial_img = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the adversarial example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the forward pass of the model to find the prediction it makes on the adversarial input and set the label to label_new. You may need to slightly tune $\\epsilon$ from above or change the original image in order to misclassify the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_new = None\n",
    "\n",
    "### BEGIN CODE ###\n",
    "\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "ax1.set_title('Original prediction: {}'.format(label_names[y_true]), fontsize=16)\n",
    "ax1.imshow(img)\n",
    "\n",
    "ax2.set_title(r'$\\vec{\\epsilon} \\cdot sign(\\nabla_x J(\\theta, \\vec{x}, y))$', fontsize=16)\n",
    "ax2.imshow(normalize(tf.squeeze(grad_x)))\n",
    "\n",
    "ax3.set_title('Adversarial prediction: {}'.format(label_new), fontsize=16)\n",
    "ax3.imshow(normalize(tf.squeeze(adversarial_img)))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "# Part 3: Implementing a Paper: ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing machine learning models, it is valuable practice to be able to read a paper, understand it conceptually, and then to actually implement its architecture. Here, we will implement a key component of the architecture from the ResNet paper: residual blocks. These residual blocks can improve model accuracy and speed up training especially in deeper neural networks. By including a shortcut connection, it becomes easier for the network to learn an optimal function starting with the identity than completely from scratch.   \n",
    "\n",
    "Read through the ResNet paper and fill in the code to implement a residual block with two layers. \n",
    "We've included a diagram from the picture for what specifically you should implement: use convolutional layers for the weight layers, and if the dimensions of $\\vec{x}$ and $\\mathcal{F}$ do not match, use a 1x1 convolution as the linear projection to match the dimensions.  \n",
    "The ResNet paper can be found here: https://arxiv.org/abs/1512.03385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"visuals/resnet_building_block.png\" alt=\"Drawing\" style=\"width: 50em;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        ### BEGIN CODE ###\n",
    "        \n",
    "        ### END CODE ###\n",
    "        \n",
    "    def call(self, x):\n",
    "        ### BEGIN CODE ###\n",
    "\n",
    "        ### END CODE ###\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sanity checking for the shapes of your outputs ###\n",
    "x = tf.random.uniform([1, 5, 5, 3])\n",
    "\n",
    "residual_block_1 = ResidualBlock(in_channels=3, out_channels=3, kernel_size=3)\n",
    "residual_block_2 = ResidualBlock(in_channels=3, out_channels=7, kernel_size=3)\n",
    "\n",
    "assert tuple(tf.shape(residual_block_1(x))) == (1, 5, 5, 3)\n",
    "assert tuple(tf.shape(residual_block_2(x))) == (1, 5, 5, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Great job! You've successfully built your own CNN to classify images, learning how you can visualize the filters and activations of a trained model along the way. You also saw a bit about how small but selective perturbations can cause your CNN to misclassify relatively easily. Finally, you were able to practice reading and implementing a machine learning research paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Sergey Ioffe, Christian Szegedy)  \n",
    "https://arxiv.org/abs/1502.03167\n",
    "\n",
    "An overview of gradient descent optimization algorithms (Sebastian Ruder)  \n",
    "https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "\n",
    "Explaining and Harnessing Adversarial Examples (Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy)  \n",
    "https://arxiv.org/abs/1412.6572\n",
    "\n",
    "Deep Residual Learning for Image Recognition (Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun)  \n",
    "https://arxiv.org/abs/1512.03385"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
